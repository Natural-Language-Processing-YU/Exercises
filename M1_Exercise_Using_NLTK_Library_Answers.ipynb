{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/Exercises/blob/main/M1_Exercise_Using_NLTK_Library_Answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe538d56",
      "metadata": {
        "id": "fe538d56"
      },
      "source": [
        "# Exercise: Text Preprocessing with NLTK\n",
        "\n",
        "## Description\n",
        "In this exercise, students will use NLTK to perform basic text preprocessing tasks on a given dataset. They will apply various NLTK functionalities to clean and normalize the words, preparing them for further analysis or modeling.\n",
        "\n",
        "## Instructions\n",
        "Introduce the concept of text preprocessing to the students, explaining its importance in natural language processing tasks. Discuss common preprocessing steps, such as tokenization, removing punctuation, converting to lowercase, and removing stopwords.\n",
        "\n",
        "Provide the students with a dataset containing a sample text document. It could be a paragraph or a collection of sentences.\n",
        "\n",
        "Instruct students to write a Python script that uses NLTK to preprocess the words in the given text document.\n",
        "\n",
        "Students should apply the following preprocessing steps using NLTK:\n",
        "\n",
        "1. **Tokenization**: Split the text into individual words or tokens.\n",
        "2. **Removing punctuation**: Remove any punctuation marks from the words.\n",
        "3. **Converting to lowercase**: Convert all words to lowercase for consistency.\n",
        "4. **Removing stopwords**: Remove common stopwords (e.g., \"the,\" \"is,\" \"and\") that do not contribute much to the meaning of the text.\n",
        "5. **Lemmatization**: Reduce words to their base or root form.\n",
        "\n",
        "Students should experiment with additional preprocessing steps or NLTK functionalities based on their understanding and familiarity with the toolkit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f3915369",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3915369",
        "outputId": "b74bea56-a067-4dab-9969-eb65fe97a0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from string import punctuation\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d8e51e32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8e51e32",
        "outputId": "16ea5ecd-7606-45e6-f4e1-de747c0d4f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence', 'text', 'preprocessing']\n"
          ]
        }
      ],
      "source": [
        "# Define the preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Removing punctuation and converting to lowercase\n",
        "    tokens = [token.lower() for token in tokens if token not in punctuation]\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Test the preprocessing function with a sample text\n",
        "text = \"This is a sample sentence for text preprocessing.\"\n",
        "preprocessed_tokens = preprocess_text(text)\n",
        "print(preprocessed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0dc26ac",
      "metadata": {
        "id": "e0dc26ac"
      },
      "source": [
        "## Explanation\n",
        "\n",
        "### Tokenization\n",
        "Tokenization is the process of splitting text into individual words or tokens. This helps in breaking down the text into manageable pieces for further processing.\n",
        "\n",
        "### Removing Punctuation and Converting to Lowercase\n",
        "Removing punctuation ensures that punctuation marks do not interfere with text analysis. Converting to lowercase makes the text uniform, so that \"Text\" and \"text\" are treated as the same word.\n",
        "\n",
        "### Removing Stopwords\n",
        "Stopwords are common words (e.g., \"the,\" \"is,\" \"and\") that do not add significant meaning to the text. Removing them helps in focusing on the words that contribute to the meaning of the text.\n",
        "\n",
        "### Lemmatization\n",
        "Lemmatization reduces words to their base or root form (e.g., \"running\" to \"run\"). This helps in normalizing the text for better analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "748faf63",
      "metadata": {
        "id": "748faf63"
      },
      "source": [
        "## Additional Examples and Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3de8ff63",
      "metadata": {
        "id": "3de8ff63"
      },
      "source": [
        "### Example 1: Stemming\n",
        "Stemming is the process of reducing words to their root form. It is different from lemmatization in that it may not produce real words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1a360a90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a360a90",
        "outputId": "6ee057e7-3c58-4652-cb1c-bc0959f9366d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'is', 'a', 'demonstr', 'of', 'stem', 'word', 'like', 'run', ',', 'runner', ',', 'and', 'run', '.']\n"
          ]
        }
      ],
      "source": [
        "# Import the PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Define the stemming function\n",
        "def stemming_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = word_tokenize(text)\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "    return stems\n",
        "\n",
        "# Test the stemming function with a sample text\n",
        "text = \"This is a demonstration of stemming words like running, runner, and runs.\"\n",
        "stemmed_tokens = stemming_text(text)\n",
        "print(stemmed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "165d59a9",
      "metadata": {
        "id": "165d59a9"
      },
      "source": [
        "### Example 2: POS Tagging\n",
        "Part-of-Speech (POS) tagging is the process of labeling words with their corresponding part of speech, such as noun, verb, adjective, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8fa96974",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fa96974",
        "outputId": "711a458f-4c25-438d-92f0-f621834fa7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('POS', 'NNP'), ('tagging', 'VBG'), ('example', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "# Import POS tagging\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Define the POS tagging function\n",
        "def pos_tagging_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    return pos_tags\n",
        "\n",
        "# Test the POS tagging function with a sample text\n",
        "text = \"This is a simple POS tagging example.\"\n",
        "pos_tags = pos_tagging_text(text)\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60f4fb8f",
      "metadata": {
        "id": "60f4fb8f"
      },
      "source": [
        "### Exercise 1: Named Entity Recognition\n",
        "Use NLTK to identify named entities in a given text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "392ef09b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "392ef09b",
        "outputId": "dc5462c6-7df7-498c-e3b7-59b0a77c7723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  44th/JJ\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import NER\n",
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Define the NER function\n",
        "def named_entity_recognition(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    named_entities = ne_chunk(pos_tags)\n",
        "    return named_entities\n",
        "\n",
        "# Test the NER function with a sample text\n",
        "text = \"Barack Obama was the 44th President of the United States.\"\n",
        "named_entities = named_entity_recognition(text)\n",
        "print(named_entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed846e4a",
      "metadata": {
        "id": "ed846e4a"
      },
      "source": [
        "### Exercise 2: Text Normalization\n",
        "Normalize the text by removing numbers and special characters, and by correcting common typos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "df3d2e77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df3d2e77",
        "outputId": "a9ab5cba-4b8c-4306-f0be-0cc39935c695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'text', 'with', 'numbers', 'and', 'special', 'characters', 'and', 'a', 'common', 'typo', 'the']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Define the text normalization function\n",
        "def normalize_text(text):\n",
        "    # Remove numbers and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Correct common typos (e.g., \"teh\" to \"the\")\n",
        "    text = re.sub(r'\\bteh\\b', 'the', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Test the normalization function with a sample text\n",
        "text = \"This is a text with numbers 123 and special characters !@# and a common typo teh.\"\n",
        "normalized_tokens = normalize_text(text)\n",
        "print(normalized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Text Preprocessing Pipeline\n",
        "\n",
        "### Objective\n",
        "\n",
        "Create a complete text preprocessing pipeline that incorporates all the preprocessing steps learned. The pipeline should take a raw text input and output the fully processed text along with various annotations (like POS tags and named entities).\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Define a function `preprocess_pipeline` that takes a raw text input.\n",
        "2. The function should apply the following preprocessing steps using NLTK:\n",
        "   - Tokenization\n",
        "   - Removing punctuation\n",
        "   - Converting to lowercase\n",
        "   - Removing stopwords\n",
        "   - Lemmatization\n",
        "   - Stemming\n",
        "   - POS tagging\n",
        "   - Named entity recognition\n",
        "   - Text normalization (removing numbers and special characters, correcting common typos)\n",
        "3. The function should return:\n",
        "   - Processed tokens\n",
        "   - POS tags\n",
        "   - Named entities"
      ],
      "metadata": {
        "id": "cbvzrBHOjRfR"
      },
      "id": "cbvzrBHOjRfR"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from string import punctuation\n",
        "import re\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_pipeline(text):\n",
        "    # Text normalization: remove numbers and special characters, correct common typos\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\bteh\\b', 'the', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Removing punctuation and converting to lowercase\n",
        "    tokens = [token.lower() for token in tokens if token not in punctuation]\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "\n",
        "    # POS tagging\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Named entity recognition\n",
        "    named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "    return {\n",
        "        'processed_tokens': stemmed_tokens,\n",
        "        'pos_tags': pos_tags,\n",
        "        'named_entities': named_entities\n",
        "    }\n",
        "\n",
        "# Test the preprocessing pipeline with a sample text\n",
        "text = \"Barack Obama was the 44th President of the United States. He was born in Hawaii and loves eating pineapple.\"\n",
        "result = preprocess_pipeline(text)\n",
        "print(\"Processed Tokens:\", result['processed_tokens'])\n",
        "print(\"POS Tags:\", result['pos_tags'])\n",
        "print(\"Named Entities:\", result['named_entities'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJBfPjtpjQTz",
        "outputId": "5d49329c-19f2-4f2d-f120-28dbc570482c"
      },
      "id": "YJBfPjtpjQTz",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Tokens: ['barack', 'obama', 'th', 'presid', 'unit', 'state', 'born', 'hawaii', 'love', 'eat', 'pineappl']\n",
            "POS Tags: [('barack', 'NN'), ('obama', 'NN'), ('th', 'NN'), ('president', 'NN'), ('united', 'JJ'), ('states', 'NNS'), ('born', 'VBP'), ('hawaii', 'JJ'), ('loves', 'NNS'), ('eating', 'VBG'), ('pineapple', 'NN')]\n",
            "Named Entities: (S\n",
            "  barack/NN\n",
            "  obama/NN\n",
            "  th/NN\n",
            "  president/NN\n",
            "  united/JJ\n",
            "  states/NNS\n",
            "  born/VBP\n",
            "  hawaii/JJ\n",
            "  loves/NNS\n",
            "  eating/VBG\n",
            "  pineapple/NN)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}