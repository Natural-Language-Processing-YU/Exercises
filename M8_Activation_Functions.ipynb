{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBx6Y1bdJoq+2IIZP6jqRe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/Exercises/blob/main/M8_Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Activation Functions Overview\n",
        "Activation functions play a crucial role in neural networks by introducing non-linearity, enabling the network to learn complex relationships between input and output. Here's a set of exercises for activation functions in Python, along with explanations:"
      ],
      "metadata": {
        "id": "PtxRQLzH8Hv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 Sigmoid Activation Function:\n",
        "The sigmoid function maps any input value to a value between 0 and 1, making it suitable for binary classification problems.\n",
        "\n",
        "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
        "*italicized text*\n",
        "\n",
        "\n",
        "Range: (0, 1)\n",
        "Used in: Binary classification problems"
      ],
      "metadata": {
        "id": "sZE8O99-8iv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX4-na2z8AIG",
        "outputId": "926180d9-dc60-4c13-fd4b-8e5394b778bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Test the sigmoid function\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "print(sigmoid(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0 ReLU (Rectified Linear Unit) Activation Function:\n",
        "\n",
        "ReLU is a popular activation function that introduces non-linearity and helps alleviate the vanishing gradient problem.\n",
        "\n",
        "$ReLU(x) = \\max(0, x)$\n",
        "\n",
        "Range: [0, +∞)\n",
        "Used in: Hidden layers of most neural networks due to its simplicity and efficiency"
      ],
      "metadata": {
        "id": "tCiDTSQk8v5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Test the ReLU function\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "print(relu(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwZUfpk783vB",
        "outputId": "0f9423c1-0f40-4a2d-aff3-d465eae5dfb6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#3.0 Leaky ReLU Activation Function:\n",
        "Leaky ReLU is similar to ReLU but allows a small negative slope for negative input values, preventing the \"dying ReLU\" problem\n",
        "\n",
        "$\\text{Leaky ReLU}(x) = \\begin{cases}\n",
        "x, & \\text{if } x > 0 \\\\\n",
        "\\alpha x, & \\text{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "\n",
        "Range: (-∞, +∞)\n",
        "Used in: Variants of ReLU to prevent the \"dying ReLU\" problem\n"
      ],
      "metadata": {
        "id": "a9bP0L5K8_F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.maximum(alpha * x, x)\n",
        "\n",
        "# Test the Leaky ReLU function\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "print(leaky_relu(x))"
      ],
      "metadata": {
        "id": "3NmFKSAC8-Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.0 Tanh (Hyperbolic Tangent) Activation Function:\n",
        "The tanh function maps input values to a range between -1 and 1, making it useful for classification problems.\n",
        "\n",
        "$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n",
        "\n",
        "\n",
        "Range: (-1, 1)\n",
        "Used in: Hidden layers and output layers for classification problems\n"
      ],
      "metadata": {
        "id": "S3T01EUI9Hi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Test the tanh function\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "print(tanh(x))"
      ],
      "metadata": {
        "id": "vfoJSZR89F8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.0 Softmax Activation Function:\n",
        "Softmax is used primarily in the output layer for multiclass classification problems. It converts raw scores to probability distribution.\n",
        "\n",
        "$\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$\n",
        "\n",
        "Range: (0, 1) and the sum of all elements in the output vector is 1\n",
        "Used in: Multiclass classification problems (output layer)"
      ],
      "metadata": {
        "id": "P_-mTAWO9Mvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # For numerical stability\n",
        "    return exp_x / exp_x.sum()\n",
        "\n",
        "# Test the softmax function\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "print(softmax(x))"
      ],
      "metadata": {
        "id": "TkSmmcl98554"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}