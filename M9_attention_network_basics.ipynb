{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTwzzaSs5pOCqkcui5qkov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/Exercises/blob/main/M9_attention_network_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.0 Introduction to Attention Network Exercise\n",
        "\n",
        "## Overview:\n",
        "In this exercise, we will explore the concept of attention in neural networks, particularly in the context of the Attention Network. Attention mechanisms have proven to be highly effective in handling sequential data, allowing the model to focus on important parts of the input while ignoring less relevant information.\n",
        "\n",
        "The Attention Network uses the concept of \"queries,\" \"keys,\" and \"values\" to calculate attention scores that indicate the relevance of each element in the input sequence. The attention scores are then used to generate a weighted sum of the input sequence, which captures the most critical information based on the attention mechanism.\n",
        "\n",
        "## Formula for Attention Scores:\n",
        "To calculate the attention scores, we use the dot product between the query vector (Q) and each key vector (K_i) in the input sequence:\n",
        "\n",
        "\\\n",
        "$ \\text{Attention Scores (A)} = Q \\cdot K_i$\n",
        "\n",
        "\\\n",
        "\n",
        "The softmax function is then applied to the attention scores to obtain attention weights (W), representing the importance of each element in the input sequence:\n",
        "\n",
        "\\\n",
        "$\\text{Attention Weights (W)} = \\text{softmax}(\\text{Attention Scores (A)}) ]$\n",
        "\n",
        "\n",
        "\\\n",
        "Finally, the weighted sum (S) is computed as the dot product between the attention weights and each value vector (V_i) in the input sequence:\n",
        "\n",
        "\\\n",
        "$\\text{Weighted Sum (S)} = W \\cdot V_i $\n",
        "\n",
        "\n",
        "\\\n",
        "\n"
      ],
      "metadata": {
        "id": "eekrtVVpKbGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Objective:\n",
        "In this exercise, we will implement a simple Attention Network using a small input sequence and visualize how the attention mechanism assigns importance to each element in the sequence. By the end of the exercise, you will gain a deeper understanding of how attention works in neural networks and its significance in processing sequential data.\n",
        "\n",
        "Let's proceed with the implementation and exploration of the Attention Network!\n"
      ],
      "metadata": {
        "id": "5UtSgB5TLN3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Define the Input Sequence\n",
        "Let's start with a small input sequence represented as a list of numbers."
      ],
      "metadata": {
        "id": "A9Os0GfiLUZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NSP46DMmJgf0"
      },
      "outputs": [],
      "source": [
        "input_sequence = [1, 2, 3, 4, 5]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Convert Input Sequence to Embeddings\n",
        "We'll represent each element of the input sequence as an embedding using a simple embedding table."
      ],
      "metadata": {
        "id": "-QcYMhI1Lao6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_table = {\n",
        "    1: [0.1, 0.2, 0.3],\n",
        "    2: [0.4, 0.5, 0.6],\n",
        "    3: [0.7, 0.8, 0.9],\n",
        "    4: [1.0, 1.1, 1.2],\n",
        "    5: [1.3, 1.4, 1.5]\n",
        "}\n",
        "\n",
        "embedded_sequence = [embedding_table[num] for num in input_sequence]\n"
      ],
      "metadata": {
        "id": "sh4NcnCmLc0A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Define Key and Value Vectors\n",
        "Now, we'll define the key and value vectors for each element in the input sequence. For simplicity, we'll use the same embedding values as the key and value vectors."
      ],
      "metadata": {
        "id": "V8je0K-2Lew9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key_vectors = embedded_sequence\n",
        "value_vectors = embedded_sequence"
      ],
      "metadata": {
        "id": "RhCh24DaLeX5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Calculate Attention Scores\n",
        "Next, we'll calculate the attention scores using the dot product between the query vector and each key vector."
      ],
      "metadata": {
        "id": "qPI8m2rbLhrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = [0.5, 0.5, 0.5]  # The query vector for attention calculation\n",
        "\n",
        "attention_scores = np.dot(query, np.array(key_vectors).T)"
      ],
      "metadata": {
        "id": "f08p_PxpLiBd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Apply Softmax to Get Attention Weights\n",
        "We'll apply the softmax function to obtain the attention weights."
      ],
      "metadata": {
        "id": "Zgh4QpORLlKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "attention_weights = softmax(attention_scores)\n",
        "print(attention_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjQ5hY7CLoLI",
        "outputId": "f50186df-4986-48da-f12c-e54c607726ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.06695687 0.10500927 0.16468731 0.25828112 0.40506543]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Calculate Weighted Sum\n",
        "The weighted sum is the final output of the Attention Network. It is the result of multiplying each value vector with its corresponding attention weight and summing up the results."
      ],
      "metadata": {
        "id": "d89_Jyr8Lviz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_sum = np.dot(attention_weights, np.array(value_vectors))\n"
      ],
      "metadata": {
        "id": "53JsPrLSLxPn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7: Final Output\n",
        "The final output of the Attention Network is the weighted sum, which represents the aggregated information from the input sequence, taking into account the attention weights."
      ],
      "metadata": {
        "id": "o2q5J7MHLxcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input Sequence:\", input_sequence)\n",
        "print(\"Attention Weights:\", attention_weights)\n",
        "print(\"Weighted Sum:\", weighted_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNq4fzvoLzbu",
        "outputId": "23bfba35-2e0f-4b5e-d328-ab57d8f91135"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: [1, 2, 3, 4, 5]\n",
            "Attention Weights: [0.06695687 0.10500927 0.16468731 0.25828112 0.40506543]\n",
            "Weighted Sum: [0.9488467 1.0488467 1.1488467]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# So, what does this mean?\n",
        "The weighted sum output in an Attention Network provides a way to focus on the most important parts of the input sequence. It calculates a new representation of the input sequence, where each element in the sequence is given a weight or importance based on its relevance to a specific query.\n",
        "\n",
        "Imagine the input sequence as a list of information, such as words in a sentence or numbers in a dataset. The query represents a specific question or target we want to focus on.\n",
        "\n",
        "The Attention Network looks at the query and the input sequence together. It evaluates how much attention each element in the input sequence deserves with respect to the query. The attention mechanism assigns higher weights to elements that are more relevant to the query and lower weights to less relevant elements.\n",
        "\n",
        "The weighted sum output is the final result, and it's calculated by combining the values from the input sequence, each multiplied by its respective attention weight. This means that elements with higher attention weights will contribute more to the weighted sum, while elements with lower attention weights will contribute less.\n",
        "\n",
        "The output, or weighted sum, is a condensed and focused representation of the input sequence. It highlights the essential information that relates to the query, allowing the model to concentrate on what matters the most for the specific task at hand.\n",
        "\n",
        "By using the attention mechanism and generating the weighted sum, the Attention Network enables more effective and targeted processing of sequential data, making it a powerful tool for natural language processing, machine translation, sentiment analysis, and other tasks involving sequences of data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HlUyKRPwL_s4"
      }
    }
  ]
}