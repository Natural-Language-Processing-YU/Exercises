{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSR/ULaRH8uXo0yNtKr/rc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/Exercises/blob/main/Exercise_N_Gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise: Text Preprocessing with NLTK\n",
        "Recall that n-grams is a language model that allows us to tokenize sequences of words in a corpora. This is the basis of the language model where we eventually calculate the probability that a word will appear given a history of words."
      ],
      "metadata": {
        "id": "Ig1RLWT11pOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load relevant libraries and download packages"
      ],
      "metadata": {
        "id": "7dakJDc6S_iI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjL4Z_Yw0FsZ",
        "outputId": "7c1a26bd-c0eb-45dc-989a-e126a49e66cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import MLE\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
        "from nltk.lm.preprocessing import flatten\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocess the sentences\n",
        "\n",
        "First Tokenize, then add padding."
      ],
      "metadata": {
        "id": "rryPOcuM4JZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "corpus = [\n",
        "    \"I love to eat pizza.\",\n",
        "    \"I love to play soccer.\",\n",
        "    \"I love to read books.\",\n",
        "    \"I love to create algorithms.\",\n",
        "    \"I hate to develop in Java, but sometimes I enjoy it on a Sunday with a coffee.\",\n",
        "\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzDPin1z4CRE",
        "outputId": "89a4487b-a06b-4c69-a7b6-f25da54249ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'love', 'to', 'eat', 'pizza', '.'], ['i', 'love', 'to', 'play', 'soccer', '.'], ['i', 'love', 'to', 'read', 'books', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In natural language processing (NLP), padding is a technique used to ensure that all sequences or sentences in a dataset have the same length. It involves adding special tokens or symbols to the beginning or end of a sequence to make it match the desired length. The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc.\n"
      ],
      "metadata": {
        "id": "r9H-UjaV4WfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "train_data, padded_sents = padded_everygram_pipeline(n,tokenized_corpus)\n",
        "print(padded_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l4VBy1J4V4d",
        "outputId": "7b573a39-a598-4315-9a39-688a8ad971a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<s>', '<s>', 'i', 'love', 'to', 'eat', 'pizza', '.', '</s>', '</s>'], ['<s>', '<s>', 'i', 'love', 'to', 'play', 'soccer', '.', '</s>', '</s>'], ['<s>', '<s>', 'i', 'love', 'to', 'read', 'books', '.', '</s>', '</s>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create the n-gram model\n"
      ],
      "metadata": {
        "id": "GQTYmz_L4mVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of NLP, flattening refers to the process of converting a nested list or sequence into a single flat list. It involves combining all the elements from the nested structure into a single-level list."
      ],
      "metadata": {
        "id": "cEOcg-eT5RJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
        "model.fit(train_data, padded_sents)"
      ],
      "metadata": {
        "id": "HRWI_2La60tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "def generate_sent(model, num_words, random_seed=42):\n",
        "    \"\"\"\n",
        "    :param model: An ngram language model from `nltk.lm.model`.\n",
        "    :param num_words: Max no. of words to generate.\n",
        "    :param random_seed: Seed value for random.\n",
        "    \"\"\"\n",
        "    content = []\n",
        "    for token in model.generate(num_words, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)"
      ],
      "metadata": {
        "id": "TyvcmhvBS3lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generate your test your result"
      ],
      "metadata": {
        "id": "lxtnq0cWS47J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(model, num_words=20, random_seed=42)"
      ],
      "metadata": {
        "id": "DkFQCnBOS4Z-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}