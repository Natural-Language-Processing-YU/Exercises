{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/Exercises/blob/main/M3_exercise_naive_bayes_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E00r5TlZnDTw"
      },
      "source": [
        "# Introduction to Naïve Bayes and Sentiment Classification\n",
        "\n",
        "## Objective:\n",
        "Students will understand the Naïve Bayes algorithm, its application in Natural Language Processing (NLP), particularly for sentiment classification, and will be able to implement a basic Naïve Bayes classifier in Python.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juVPS1jRnDT0"
      },
      "source": [
        "## Naïve Bayes Algorithm\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes' theorem, which calculates the probability of a class given the presence of features. It is called \"naïve\" because it assumes that the features are conditionally independent of each other given the class label, which is rarely true in practice but simplifies the calculations significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-XxhokonDT0"
      },
      "source": [
        "### Bayes' Theorem\n",
        "Bayes' theorem is stated as:\n",
        "\n",
        "$$ P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)} $$\n",
        "\n",
        "Where:\n",
        "- $ P(C|X) $ is the posterior probability of class $ C $ given the feature vector $ X $.\n",
        "- $ P(X|C) $ is the likelihood of feature vector $ X $ given class $ C $.\n",
        "- $ P(C) $ is the prior probability of class $ C $.\n",
        "- $ P(X) $ is the probability of the feature vector $ X $ (which can be considered a normalization factor)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyUAjnQvnDT1"
      },
      "source": [
        "## Naïve Bayes Classifier\n",
        "In the context of text classification, documents are represented as feature vectors. The Naïve Bayes classifier calculates the probability of each class given the document's features and assigns the document to the class with the highest probability.\n",
        "\n",
        "### Steps in Naïve Bayes Classification:\n",
        "\n",
        "1. **Training Phase:**\n",
        "    - **Calculate Prior Probabilities:** The prior probability of each class $ P(C) $ is calculated based on the training data.\n",
        "    - **Calculate Likelihood:** The likelihood $ P(X|C) $ is calculated for each feature (word) in the vocabulary given each class. This is typically done using the frequency of the word in documents of the given class.\n",
        "\n",
        "2. **Classification Phase:**\n",
        "    - For a new document, the classifier computes the posterior probability for each class using the features present in the document.\n",
        "    - The document is assigned to the class with the highest posterior probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPzHJ999nDT1"
      },
      "source": [
        "## Example: Sentiment Analysis\n",
        "Consider a simple example of classifying movie reviews as positive or negative. Suppose we have the following training data:\n",
        "\n",
        "| Sentence                   | Label    |\n",
        "|----------------------------|----------|\n",
        "| \"The movie was great!\"     | Positive |\n",
        "| \"I didn't like the movie.\" | Negative |\n",
        "| \"The acting was fantastic!\"| Positive |\n",
        "| \"It was a terrible movie.\" | Negative |\n",
        "\n",
        "1. **Calculate Priors:**\n",
        "$$ P(\\text{Positive}) = \\frac{2}{4} = 0.5 $$\n",
        "$$ P(\\text{Negative}) = \\frac{2}{4} = 0.5 $$\n",
        "\n",
        "2. **Calculate Likelihoods:**\n",
        "For each word in the vocabulary, calculate the likelihoods given the class. For example:\n",
        "$$ P(\\text{great}|\\text{Positive}) = \\frac{1}{4} = 0.25 $$\n",
        "$$ P(\\text{terrible}|\\text{Negative}) = \\frac{1}{4} = 0.25 $$\n",
        "\n",
        "3. **Classification:**\n",
        "For a new sentence, e.g., \"The acting was great\":\n",
        "$$ P(\\text{Positive}|\\text{The acting was great}) = P(\\text{Positive}) \\cdot P(\\text{The}|\\text{Positive}) \\cdot P(\\text{acting}|\\text{Positive}) \\cdot P(\\text{was}|\\text{Positive}) \\cdot P(\\text{great}|\\text{Positive}) $$\n",
        "$$ = 0.5 \\cdot 0.5 \\cdot 0.5 \\cdot 0.5 \\cdot 0.25 = 0.015625 $$\n",
        "\n",
        "Similarly, calculate for the Negative class and compare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoCc0au9nDT1"
      },
      "source": [
        "### Advantages:\n",
        "- **Simplicity:** Easy to implement and understand.\n",
        "- **Efficiency:** Works well with large datasets.\n",
        "- **Scalability:** Scales linearly with the number of features and data points.\n",
        "\n",
        "### Limitations:\n",
        "- **Conditional Independence Assumption:** The assumption that features are independent given the class label is rarely true in practice.\n",
        "- **Data Scarcity:** Performance can be poor if the training data is limited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk2eAAvYnDT1"
      },
      "source": [
        "## Implementation in Python\n",
        "Here is a basic example of implementing Naïve Bayes for sentiment analysis using Python and the `sklearn` library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri8qd0HOnDT1"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data\n",
        "X_train = [\"The movie was great!\", \"I didn't like the movie.\", \"The acting was fantastic!\", \"It was a terrible movie.\"]\n",
        "y_train = [\"Positive\", \"Negative\", \"Positive\", \"Negative\"]\n",
        "\n",
        "X_test = [\"The movie was awesome!\", \"I hated the movie.\"]\n",
        "\n",
        "# Vectorize the data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naïve Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_counts, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = clf.predict(X_test_counts)\n",
        "\n",
        "# Output results\n",
        "for doc, category in zip(X_test, y_pred):\n",
        "    print(f'{doc} => {category}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3Yz5Q8fnDT2"
      },
      "source": [
        "By following this lesson plan and detailed explanation, students will gain a solid understanding of Naïve Bayes and its application in sentiment classification."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}