{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/Exercises/blob/main/M8_Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f654053f"
      },
      "source": [
        "\n",
        "# Activation Functions Overview\n",
        "\n",
        "Activation functions play a crucial role in neural networks by introducing non-linearity, enabling the network to learn complex relationships between input and output. Here's a set of exercises for activation functions in Python, along with explanations, visualizations, and real-world examples:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b908c1fa"
      },
      "source": [
        "\n",
        "## 1.0 Sigmoid Activation Function\n",
        "\n",
        "The sigmoid function maps any input value to a value between 0 and 1, making it suitable for binary classification problems.\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "Range: (0, 1)  \n",
        "Used in: Binary classification problems\n",
        "\n",
        "### Definition and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5ec67b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Test the sigmoid function\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = sigmoid(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title('Sigmoid Activation Function')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9629b9a"
      },
      "source": [
        "\n",
        "### Real-World Example\n",
        "The sigmoid function is commonly used in logistic regression models for binary classification problems, such as determining whether an email is spam or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56ac5af0"
      },
      "source": [
        "\n",
        "## 2.0 ReLU (Rectified Linear Unit) Activation Function\n",
        "\n",
        "ReLU is a popular activation function that introduces non-linearity and helps alleviate the vanishing gradient problem.\n",
        "\n",
        "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
        "\n",
        "Range: [0, +∞)  \n",
        "Used in: Hidden layers of most neural networks due to its simplicity and efficiency\n",
        "\n",
        "### Definition and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1d7fdbe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Test the ReLU function\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = relu(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title('ReLU Activation Function')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9934a97f"
      },
      "source": [
        "\n",
        "### Real-World Example\n",
        "ReLU is extensively used in convolutional neural networks (CNNs) for image recognition tasks, such as classifying objects in images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d3fff05"
      },
      "source": [
        "\n",
        "## 3.0 Leaky ReLU Activation Function\n",
        "\n",
        "Leaky ReLU is a variation of ReLU that allows a small, non-zero gradient when the input is negative.\n",
        "\n",
        "$$\\text{Leaky ReLU}(x) = \\begin{cases}\n",
        "x, & \\text{if } x > 0 \\\\\n",
        "\\alpha x, & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "Range: (-∞, +∞)  \n",
        "Used in: Variants of ReLU to prevent the \"dying ReLU\" problem\n",
        "\n",
        "### Definition and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "453ef08d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "# Test the Leaky ReLU function\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = leaky_relu(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title('Leaky ReLU Activation Function')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de8b1c0e"
      },
      "source": [
        "\n",
        "### Real-World Example\n",
        "Leaky ReLU is used in generative adversarial networks (GANs) to avoid the dying ReLU problem and improve the training stability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "551aa300"
      },
      "source": [
        "\n",
        "## 4.0 Tanh (Hyperbolic Tangent) Activation Function\n",
        "\n",
        "The tanh function maps input values to a range between -1 and 1, making it useful for classification problems.\n",
        "\n",
        "$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
        "\n",
        "Range: (-1, 1)  \n",
        "Used in: Hidden layers and output layers for classification problems\n",
        "\n",
        "### Definition and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5433aade"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Test the tanh function\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = tanh(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title('Tanh Activation Function')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2394084"
      },
      "source": [
        "\n",
        "### Real-World Example\n",
        "The tanh function is often used in recurrent neural networks (RNNs) for natural language processing tasks, such as sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3829851"
      },
      "source": [
        "\n",
        "## 5.0 Softmax Activation Function\n",
        "\n",
        "Softmax is used primarily in the output layer for multiclass classification problems. It converts raw scores to probability distribution.\n",
        "\n",
        "$$\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
        "\n",
        "Range: (0, 1) and the sum of all elements in the output vector is 1  \n",
        "Used in: Multiclass classification problems (output layer)\n",
        "\n",
        "### Definition and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5da1784"
      },
      "outputs": [],
      "source": [
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # For numerical stability\n",
        "    return exp_x / exp_x.sum()\n",
        "\n",
        "# Test the softmax function\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = softmax(x)\n",
        "\n",
        "plt.bar(range(len(x)), y)\n",
        "plt.title('Softmax Activation Function')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Probability')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdb3d844"
      },
      "source": [
        "\n",
        "### Real-World Example\n",
        "Softmax is widely used in neural networks for multiclass classification tasks, such as classifying handwritten digits in the MNIST dataset.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}